{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory 2.5: Logistic regression + regularization\n",
    "\n",
    "Here we will construct our first instance of the **logistic regression classifier** and couple it with some ideas regarding **regularization**. As per usual, you will need the `synthetic_dataset.csv` present in the .zip file you downloaded alongside this notebook. In this case, you also have a `utils.py` file that contains several functions to diagnose your model. Check the inner contents of that file (you will see that there we have some functions you already constructed in previous labs, mostly related to the characterization of the fit). \n",
    "\n",
    "In addition, we will be using the following libraries:\n",
    "- Data management:\n",
    "    - [numpy](https://numpy.org/)\n",
    "    - [pandas](https://pandas.pydata.org/)\n",
    "    - [scipy](https://scipy.org/) \n",
    "- Modelling and scoring:\n",
    "    - [scikit-learn](https://scikit-learn.org)\n",
    "- Plotting:\n",
    "    - [seaborn](https://seaborn.pydata.org/)\n",
    "    - [matplotlib](https://matplotlib.org/)\n",
    "    \n",
    "### **All the things you need to do are marked by a \"TODO\" comment nearby. Make sure you *read carefully everything before working* and solve each point before submitting your solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add it to sys.path\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# From the \"utils.py\" file you will only need this...\n",
    "from src.utils import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the class for the logistic regression model. In here, we will recover the same structure we have followed thus far in the course, implementing the same models we have used before but now in this new context. \n",
    "\n",
    "Here you can reuse the different functions you have implemented in previous notebooks to make your life easier (and, in fact, you will _need_ some of those functions here in order to make this model work).\n",
    "\n",
    "**Before you start to work here, please read everything carefully, including the comments describing the inner workings of each method implemented. _Do not rush into writing code_, do things in an ordered fashion.**\n",
    "\n",
    "For the initial part of the lab you will only need to complete the `.fit`, `.log_likelihood` and the `.sigmoid` method, since you will not perform regularization just yet. **Complete the code of this class as you need it**: Fill the `.fit` and the basic related methods as soon as you need it for later cells, leaving the regularization-related methods for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Lab2_5_LogisticRegression_and_regularization import LogisticRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us import the dataset, as usual. Load the `synthetic_data.csv` file and split it into `X` (inputs) and `y` (output). The input variables are named as `input_XX` and the (sole) target variable is named `target`. As you will see, it is a dataset where the target variable is binary categorical, coded with `1`s and `0`s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'synthetic_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynthetic_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# TODO: Separate the X and y values\u001b[39;00m\n\u001b[0;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Javier\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'synthetic_dataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"synthetic_dataset.csv\", sep = \",\")\n",
    "\n",
    "# TODO: Separate the X and y values\n",
    "X = df.drop(columns='target') \n",
    "y = df['target']\n",
    "\n",
    "print(df.head(1))\n",
    "\n",
    "# Convert the pandas dataframes into np.arrays so that we can use all the previous codes we defined\n",
    "# Warning: Watch out for the dimensions!!\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can employ previous code you developed in order to split the data into the two sets for training and testing. In this case, as we have done before, _we will purposefully forget about the validation set_ (it will come back next time, we promise...)\n",
    "\n",
    "For now, we will give you the code to split the data into train/test using the proportion indicated via the `test_size` parameter. **_However, be sure to understand what is being done here, do not dismiss this step._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, stratify=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Splits arrays or matrices into random train and test subsets. This function demonstrates how to \n",
    "    divide a dataset into training and testing sets, optionally stratifying the samples and ensuring \n",
    "    reproducibility with a random state.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features matrix, where rows represent samples and columns represent features.\n",
    "    - y (np.ndarray): Target labels array, aligned with the samples in X.\n",
    "    - test_size (float or int): Determines the size of the test set. If float, it represents a proportion \n",
    "                                of the dataset; if int, it specifies the number of samples.\n",
    "    - stratify (np.ndarray): If provided, the function will ensure the class proportions in train and test \n",
    "                             sets mirror those of the provided array, typically the target labels array.\n",
    "    - random_state (int): Seed for the random number generator to ensure reproducible splits.\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: Arrays containing the split of features and labels into training and \n",
    "                                        test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the seed for reproducibility\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Determine the number of samples to allocate to the test set\n",
    "    n_samples = X.shape[0]\n",
    "    if isinstance(test_size, float):\n",
    "        n_test = int(n_samples * test_size)\n",
    "    else:\n",
    "        n_test = test_size\n",
    "    n_train = n_samples - n_test\n",
    "\n",
    "    # Create an array of indices and shuffle if not stratifying\n",
    "    indices = np.arange(n_samples)\n",
    "    if stratify is None:\n",
    "        np.random.shuffle(indices)\n",
    "    else:\n",
    "        # For stratified splitting, determine the distribution of classes\n",
    "        unique_classes, y_indices = np.unique(stratify, return_inverse=True)\n",
    "        class_counts = np.bincount(y_indices)\n",
    "        test_counts = np.round(class_counts * test_size).astype(int)\n",
    "\n",
    "        # Allocate indices to train and test sets preserving class distribution\n",
    "        train_indices, test_indices = [], []\n",
    "        for class_index in range(len(unique_classes)):\n",
    "            class_indices = indices[y_indices == class_index]\n",
    "            np.random.shuffle(class_indices)\n",
    "            boundary = test_counts[class_index]\n",
    "            test_indices.extend(class_indices[:boundary])\n",
    "            train_indices.extend(class_indices[boundary:])\n",
    "\n",
    "        # Concatenate indices to form the final split\n",
    "        indices = train_indices + test_indices\n",
    "\n",
    "    # Use the indices to partition the dataset\n",
    "    X_train = X[indices[:n_train]]\n",
    "    X_test = X[indices[n_train:]]\n",
    "    y_train = y[indices[:n_train]]\n",
    "    y_test = y[indices[n_train:]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the train/test split function, perform the split in your data in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the logistic regression model\n",
    "\n",
    "Now, it comes the turn to train the logistic regression. Remember, **this model does not have a closed-form solution for the values of the parameters, so instead you have to optimize the parameters recursively via an approximate technique**. There are many different approaches here, but we will resort to the most familiar one: _**gradient descent**_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of logistic regression model\n",
    "num_iterations = 1000      # Number of iterations to run the gradient_descent algorithm\n",
    "learning_rate = 0.01       # Learning rate parameter for gradient descent\n",
    "\n",
    "# You can try different values for these parameters here!\n",
    "# Watch out: Some values create unexpected responses of the model, so be careful..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the code, _let's get ready!_ \n",
    "\n",
    "Create an instance of your `LogisticRegressor` and run the fit. This should call the `fit` method, which should contain almost the same thing you had in the previous practice for the `fit_gradient_descent` method. You will need to tune it a bit so that it updates the parameters accordingly, but once you have it, you can run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegressor()\n",
    "LogReg.fit(X_train, y_train, learning_rate=learning_rate, num_iterations=num_iterations, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is fitted, you can try to extract the prediction probabilities for each data point. To this end, extract the predicted probabilities for associated for the predicted values of `y_train` and `y_test` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities of train and test datasets\n",
    "y_train_prob = LogReg.predict_proba(X_train)\n",
    "y_test_prob = LogReg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use these values in just a second..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit assessment\n",
    "\n",
    "In order to see how well our model fits the data, we must resort to the techniques we have constructed in previous laboratories. In particular, we will use the function `classification_report` present in `utils.py`. We strongly encourage you to check out the functions present in that file, as you will see that they represent (our version of) certain functions that may look familiar. \n",
    "\n",
    "In particular, take a look at the `classification_report` function. Before you continue, **what does this function do, how do you use it and why could it be interesting for your task here?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have answered the previous question, let us appy `classification_report` to the fit we obtained. Execute it in the next cell and check out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply the classification_report function for the **TRAIN dataset predictions**\n",
    "classification_report(None, None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply the classification_report function for the **TEST dataset predictions**\n",
    "classification_report(None, None, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you think about this results? Do you observe any important features that you think are worth mentioning?**\n",
    "\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and regularization of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have obtained our first fully-functional model of logistic regression (_congrats!_). \n",
    "\n",
    "However, in our dataset we have ___ numerical regressors and ___ categorical regressors. Thus, we have a total of ___ input variables (columns) for the model, which entails a total of ___ parameters for the logistic regression model. \n",
    "\n",
    "> Fill the gaps in the text above\n",
    "\n",
    "Since we have **not** conducted preprocessing procedure for the dataset, we may be \"doing more than we need\". In this case, we have purposefully left out the preprocessing steps so you can see the effects of the next part of this lab: **regularization**.\n",
    "\n",
    "As you may recall, regularization implies modifying the objective (loss) function a bit in order to reduce the overall complexity of the model. In most cases, complexity is translated into high weight (parameter) values, and thus penalizing large values here will help the model to keep a more reasonable balance in the *bias/variance* trade-off. \n",
    "\n",
    "In class we introduced three main regularization techniques: **ridge**, **lasso** and **elastic net** regularization. Each one of those has their special properties, which we will investigate in just a second. Keep in mind, in each case, that modifying the objective function induces a change in the gradients of the gradient descent algorithm, which is why each regularization technique is able to affect the final values for the parameters of the fit.\n",
    "\n",
    "_If you need to review the ideas behind regularization, now is the time. Make sure you get the gist of it before you continue._\n",
    "\n",
    "**To perform the things you will need in the next part, you will need to implement the methods `ridge_regularization`, `lasso_regularization` and `elastic_net_regularization` in `LogisticRegressor`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regularization is performed introducing what in the objective function?**\n",
    "\n",
    "> Write your answer here\n",
    "\n",
    "Once you have this, let us implement it already!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform regularization for different values of the regularization parameter\n",
    "# TODO: Create an array (C_values) that contains 11 values distributed in log-scale, covering from 10^(-5) to 10^5\n",
    "pow_min = -5\n",
    "pow_max = 5\n",
    "C_values = None  # Vector of \\lambda (regularization parameters)\n",
    "\n",
    "# Keep these two empty lists to contain the final weights for each regularization parameter\n",
    "weights_evolution = []    # To store the final weights for each C\n",
    "accuracies = []           # To store accuracy for each C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit a logistic regression with ridge regularization for each possible value of the regularization parameter present in `C_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in C_values:\n",
    "    model = LogisticRegressor()  # Update to include necessary parameters\n",
    "    model.fit(None, None, learning_rate = None, num_iterations = None, penalty = 'ridge', C = C)\n",
    "    weights_evolution.append(model.weights)\n",
    "    \n",
    "    # Predict and calculate accuracy in the test data\n",
    "    y_pred = model.predict(None)\n",
    "    accuracy = accuracy_score(None, None)\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to see how well the model behaves, we will plot different things we have access to. This means that we want to plot:\n",
    "* The evolution for each weight vs. the regularization parameter\n",
    "* The accuracy of the method vs. the regularization parameter.\n",
    "    - In this case, mark also the value of the regularization that achieves the highest accuracy\n",
    "    \n",
    "**What is the regularization value that achieves highest accuracy?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the evolution of weights\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot for the changes in the weights in terms of the regularization parameter\n",
    "plt.subplot(1, 2, 1)\n",
    "for feature_index in range(len(weights_evolution[0])):\n",
    "    weight_trajectory = [weights[feature_index] for weights in weights_evolution]\n",
    "    plt.plot(C_values, weight_trajectory, label=f'Feature {feature_index + 1}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Inverse of Regularization Strength)')\n",
    "plt.ylabel('Weight Magnitude')\n",
    "plt.title('Evolution of Weights with Respect to C')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "\n",
    "# TODO: Find the maximum accuracy\n",
    "max_accuracy = None\n",
    "\n",
    "# TODO: Filter the C values that correspond to the maximum accuracy\n",
    "optimum_C_values = None\n",
    "\n",
    "# Select the maximum C from those that yield the highest accuracy\n",
    "optimum_C = None\n",
    "\n",
    "print(f\"The optimum value of C based on accuracy is: {optimum_C}\")\n",
    "\n",
    "# Plotting accuracy as a function of C\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(C_values, accuracies, marker='o', linestyle='-')\n",
    "plt.plot(optimum_C, max_accuracy, 'ro', markersize=10, label='Optimum C')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Inverse of Regularization Strength)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy with Respect to C')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the model fit with the regularization parameter that induces the highest accuracy, we will create a separate object and train it in the training data with this optimum value of the regularization parameter. We will do this to ease further treatment to measure the performance of this preferred method with the `classification_report` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LogisticRegressor model with the optimum C\n",
    "LogReg_optimum_C_Ridge = LogisticRegressor()\n",
    "\n",
    "# TODO: Train the model using the training data and the optimum C\n",
    "LogReg_optimum_C_Ridge.fit(X_train, y_train, \n",
    "                           learning_rate = learning_rate, \n",
    "                           num_iterations = num_iterations, \n",
    "                           penalty = 'ridge', \n",
    "                           C = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the predictive probabilities for the `y_test`and apply the `classification_report` to check out the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the call for the report with the y_test_prob obtained from the test data\n",
    "y_test_prob = LogReg_optimum_C_Ridge.predict_proba(None)\n",
    "classification_report(None, None, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot here the differences between the weights for the non-regularized model and the Ridge-regularized one you just obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot an histogram containing the differences between the parameter values for both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you see when you compare this with the original (non-regularized) model?**\n",
    "\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso regularization is performed introducing what in the objective function?**\n",
    "\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform regularization for different values of the regularization parameter\n",
    "# TODO: Create an array (C_values) that contains 11 values distributed in log-scale, covering from 10^(-5) to 10^4\n",
    "pow_min = -5\n",
    "pow_max = 4  # In this case we maintain here a 4 instead of a 5 s.a. in the Ridge example\n",
    "\n",
    "C_values = None # Vector of \\lambda (regularization parameters)\n",
    "weights_evolution = []  # To store the final weights for each C\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, plot and display the following information:\n",
    "* The evolution for each weight vs. the regularization parameter\n",
    "* The accuracy of the method vs. the regularization parameter.\n",
    "    - In this case, mark also the value of the regularization that achieves the highest accuracy\n",
    "    \n",
    "**What is the regularization value that achieves highest accuracy?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the LogisticRegressor class and dataset are prepared\n",
    "for C in C_values:\n",
    "    # Initialize model with current C\n",
    "    model = LogisticRegressor()  # Update to include necessary parameters\n",
    "    model.fit(X_train, y_train, learning_rate=learning_rate, num_iterations=num_iterations, \n",
    "              penalty='lasso', C=C)\n",
    "    weights_evolution.append(model.weights)\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plotting the evolution of weights\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for feature_index in range(len(weights_evolution[0])):\n",
    "    weight_trajectory = [weights[feature_index] for weights in weights_evolution]\n",
    "    plt.plot(C_values, weight_trajectory, label=f'Feature {feature_index + 1}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Inverse of Regularization Strength)')\n",
    "plt.ylabel('Weight Magnitude')\n",
    "plt.title('Evolution of Weights with Respect to C')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "\n",
    "# TODO: Find the maximum accuracy\n",
    "max_accuracy = None\n",
    "\n",
    "# TODO: Filter the C values that correspond to the maximum accuracy\n",
    "optimum_C_values = None\n",
    "\n",
    "# TODO: Select the maximum C from those that yield the highest accuracy\n",
    "optimum_C = None\n",
    "\n",
    "print(f\"The optimum value of C based on accuracy is: {optimum_C}\")\n",
    "\n",
    "# Plotting accuracy as a function of C\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(C_values, accuracies, marker='o', linestyle='-')\n",
    "plt.plot(optimum_C, max_accuracy, 'ro', markersize=10, label='Optimum C')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Inverse of Regularization Strength)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy with Respect to C')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What difference do you observe when you compare with the Ridge regularization case?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, create a separate object and train it in the training data with this optimum value of the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LogisticRegressor model with the optimum C\n",
    "LogReg_optimum_C_Lasso = LogisticRegressor()\n",
    "\n",
    "# TODO: Train the model using the training data and the optimum C\n",
    "LogReg_optimum_C_Lasso.fit(X_train, y_train, \n",
    "                           learning_rate = learning_rate, \n",
    "                           num_iterations = num_iterations, \n",
    "                           penalty = 'lasso', \n",
    "                           C = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `classification_report` function and check out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the call for the report with the y_test_prob obtained from the test data\n",
    "y_test_prob = LogReg_optimum_C_Lasso.predict_proba(None)\n",
    "classification_report(None, None, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot here the differences between the weights for the non-regularized model and this new Lasso-regularized one you just obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot an histogram containing the differences between the parameter values for both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you see when you compare this with the original (non-regularized) model?**\n",
    "\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ElasticNet regularization is performed introducing what in the objective function?**\n",
    "\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform regularization for different values of the regularization parameter\n",
    "# TODO: Create an array (C_values) that contains 11 values distributed in log-scale, covering from 10^(-5) to 10^4\n",
    "pow_min = -5\n",
    "pow_max = 4  # In this case we maintain here a 4 instead of a 5 s.a. in the Ridge example\n",
    "\n",
    "C_values = None # Vector of \\lambda (regularization parameters)\n",
    "weights_evolution = []  # To store the final weights for each C\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, plot and display the following information:\n",
    "* The evolution for each weight vs. the regularization parameter\n",
    "* The accuracy of the method vs. the regularization parameter.\n",
    "    - In this case, mark also the value of the regularization that achieves the highest accuracy\n",
    "    \n",
    "**What is the regularization value that achieves highest accuracy?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the LogisticRegressor class and dataset are prepared\n",
    "for C in C_values:\n",
    "    # Initialize model with current C\n",
    "    model = LogisticRegressor()  # Update to include necessary parameters\n",
    "    model.fit(X_train, y_train, learning_rate=learning_rate, num_iterations=num_iterations, \n",
    "              penalty='elasticnet', C=C)\n",
    "    weights_evolution.append(model.weights)\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plotting the evolution of weights \n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for feature_index in range(len(weights_evolution[0])):\n",
    "    weight_trajectory = [weights[feature_index] for weights in weights_evolution]\n",
    "    plt.plot(C_values, weight_trajectory, label=f'Feature {feature_index + 1}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Inverse of Regularization Strength)')\n",
    "plt.ylabel('Weight Magnitude')\n",
    "plt.title('Evolution of Weights with Respect to C')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "\n",
    "# TODO: Find the maximum accuracy\n",
    "max_accuracy = None\n",
    "\n",
    "# TODO: Filter the C values that correspond to the maximum accuracy\n",
    "optimum_C_values = None \n",
    "\n",
    "# TODO: Select the maximum C from those that yield the highest accuracy\n",
    "optimum_C = None\n",
    "\n",
    "print(f\"The optimum value of C based on accuracy is: {optimum_C}\")\n",
    "\n",
    "# Plotting accuracy as a function of C\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(C_values, accuracies, marker='o', linestyle='-')\n",
    "plt.plot(optimum_C, max_accuracy, 'ro', markersize=10, label='Optimum C')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Inverse of Regularization Strength)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy with Respect to C')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What difference do you observe when you compare with the Ridge regularization case?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, create a separate object and train it in the training data with this optimum value of the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LogisticRegressor model with the optimum C\n",
    "LogReg_optimum_C_ElasticNet = LogisticRegressor()\n",
    "\n",
    "# TODO: Train the model using the training data and the optimum C\n",
    "LogReg_optimum_C_ElasticNet.fit(X_train, y_train, \n",
    "                                learning_rate = learning_rate, \n",
    "                                num_iterations = num_iterations, \n",
    "                                penalty = 'elasticnet', \n",
    "                                C = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `classification_report` function and check out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the call for the report with the y_test_prob obtained from the test data\n",
    "y_test_prob = LogReg_optimum_C_ElasticNet.predict_proba(None)\n",
    "classification_report(None, None, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot here the differences between the weights for the non-regularized model and this new Lasso-regularized one you just obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot an histogram containing the differences between the parameter values for both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you see when you compare this with the original (non-regularized) model?**\n",
    "\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Comparing the un-regularized model with the three other versions with regularization, what do you observe? What can you conclude about the complete analysis?**\n",
    "\n",
    "> Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
